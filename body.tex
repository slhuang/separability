

%====================================================================
\section{Introduction}
\label{sec:intro}



%\newpage
%==================================================================


\section{Problem Definition}\label{sec:prob}
In Section~\ref{sec:intro}, we have talked about different biology applications that can be abstracted as a separability problem. In this section, we will formally formulate the problem and discuss the challenges in it. 

We first introduce some essential notations. Let $\mm$ be a feature-object matrix of size $m\times N$, where each row is a feature and each column is an object. Correspondingly, denote the $m$ features as $\ff=\{f_1,f_2,\cdots,f_m\}$ and $N$ objects as $\oo=\{o_1,o_2,\cdots,o_N\}$. Each entry $\mm_{i,j}$ in $\mm$ is of numeric type, referring to the value of object $o_j$ on feature $f_i$. In addition, we are given two non-overlapping sets of objects, one with positive label and the other with negative label, denoted as $\oo_+$ and $\oo_-$ respectively. Both positive and negative objects are a subset of all objects, i.e., $\oo_+\subset \oo$ and $\oo_-\subset \oo$. Let $\widehat{\oo}$ be the object set with both positive and negative labels and $n$ be the total number of objects, i.e., $\oo_+\cup \oo_- = \widehat{\oo}$, $|\widehat{\oo}| = n$ and $n\leq N$. Furthermore, let $l_k$ be the label of object $o_k\in \widehat{\oo}$, i.e., $l_k=1$ if $o_k$ is positive and $l_k=-1$ if $o_k$ is negative. 

As illustrated in Figure~\ref{fig:workflow}, given matrix $\mm$ and two object sets $\oo_+$ and $\oo_-$, the goal is to find discriminative features to separate $\oo_+$ from $\oo_-$, and output a visualization to explain the separability. In \genviz, we focus on finding \topk feature pairs instead of \topk single features. This is because \begin{inparaenum}[\itshape (a)\upshape]
\item a single feature can be considered as a special case of a feature pair by taking the same feature in a feature pair;
\item a combined feature pair is likely to provide new insight compared to single features;
\item feature pair can be easily visualized in a 2-D space.
\end{inparaenum}
As we will illustrate in Section~\ref{sec:exp}, two features that perform poorly on their own may have very good separability when combined together. Furthermore, since we are targeting as a data exploration tool before more time-consuming machine learning methods, our design principle is to prioritize running time over accuracy. Next, let us formally define the \textit{separability} problem. 

% \begin{figure}[h]
% 	\centering
% 	\includegraphics[width=\linewidth]{fig/workflow.pdf}
% \caption{\genviz Workflow}
% \label{fig:workflow}
% \end{figure} 

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{fig/workflow2.pdf}
\caption{\genviz Workflow}
\label{fig:workflow}
\end{figure*} 




% \slhuang{feature pair is }
\begin{formulation}[Separability]\label{prob:separability}
Given a feature-object matrix $\mm$ and two labeled object set $(\oo_+,\oo_-)$, \textbf{fast identify} \topk feature pairs $(f_i,f_j)$ separating $\oo_+$ from $\oo_-$ based on a given \textbf{separability metric}, and output a 2-D \textbf{visualization}.
\end{formulation}
There are three key aspects in Problem~\ref{prob:separability}. First, the output of \genviz is not just the \topk feature pairs, but also the corresponding visualizations. The output visualizations can help the users better interpret the result, i.e., how $\oo_+$ is separated from $\oo_-$. Second, given a feature pair $(f_i,f_j)$, how to measure its quality in separating $\oo_+$ from $\oo_-$ poses an interesting challenge. Last but not least, how to fast identify \topk feature pairs based on the separability metric is also a big concern since small latency is critical for data exploration tool. In the following, we ellaborate more on these three points.

\stitle{Visualization Output.} Given a feature pair $(f_i,f_j)$, it is natural to visualize the object sets $(\oo_+,\oo_-)$ in a two dimensional space by coloring $\oo_+$ as red and $\oo_-$ as green, where the x-axis and y-axis represent feature $f_i$ and $f_j$ respectively. Thus, a visualization and a feature pair has a one-to-one relationship: each visualization corresponds to one feature pair and vice versa. By looking at the visualization, the users can easily have a general sense of how $\oo_+$ and $\oo_-$ are separated for a given feature pair, and have a better interpretation towards the results compared to the pure feature pair names.


\stitle{Separability Metric.} As far as we are concerned, existing separability measurements focus on single feature instead of feature pair. For instance, in order to characterize differentially expressed genes (DEG), biologists typically do association test (e.g, hypergeometric test) to find the best single feature that separate two gene sets. However, we argue that feature pair can provide new insights that is not revealed by top single features, as we will demonstrate in Section~\ref{sec:exp}. Hence, developing a meaningful separability metric for feature pairs is the very first step towards our separability problem and is of great importance. Furthermore, since \genviz has visualization as the output instead of a pure separability score, our proposed separability metric should encodes the visual separability to some extent. 

\stitle{Fast Identification.} Since \genviz serves as a data exploration tool before looking into more sophisticated machine learning algorithms, we in particular care more about the running time than the accuracy. For one thing, instead of complicated machine learning methods, we prefer light-weight separability metrics. In some sense, we try to solve Problem~\ref{prob:separability} in a "quick and dirty" way. For another, various optimization mechanisms are essential to further cut down the running time.

\begin{table}[t!]
\centering
\small
\begin{tabular}{c|c|c|c}
  %\hline
   Symb. & Description & Symb. & Description\\
    \hline
    \hline
    $\mm$ & feature-dample matrix & $\ff$ & feature set in $\mm$ \\
    \hline
    $f_i$ & feature $i$ in $\ff$ & $m$ & cardinality of $\ff$\\
    \hline
    $\oo$ & object set in $\mm$ & $N$ & cardinality of $\oo$\\
    \hline
    $\oo_+$ & positive object set & $\oo_-$ & negative object set\\
    \hline
    $\widehat{\oo}$ & $\oo_+\cup \oo_-$ & $n$ & cardinality of $\widehat{\oo}$\\
    \hline
    $o_k$ & object $k$ in $\widehat{\oo}$ & $l_k$ & lebel of object $o_k$\\
    \hline
    $\ell$ & separating line in 2-D & $\hat{\ell}$ & representative line  in 2-D\\
    \hline
    $\eta_{i,j}^{\ell,k}$ & estimated label of $o_k$ & $\theta_{i,j}^{\ell,k}$ & $o_k$ is correctly separated? \\
    \hline
    $\theta_{i,j}^{\ell}$ & \# correctly separated $o_k$ & $\theta_{i,j}$ & separability score\\
    \hline
    $\widehat{\mm}$ & $\mm$ after transformation &  $\tilde{\theta}_{i,j}$ & estimated $\theta_{i,j}$\\
    \hline
 \end{tabular}
\vspace{-6pt}
\caption{Notations}
\label{tbl:notation}
\vspace{-18pt}
\end{table}

In the following, we will first describe our selected separability metric in Section~\ref{sec:metric}, and then discuss different optimization techniques in Section~\ref{sec:opt}.



%==================================================================
\section{Separability Metric}\label{sec:metric}

As discussed in Section~\ref{sec:prob}, given a feature pair $(f_i,f_j)$ we can visualize $\oo_+$ and $\oo_-$ in a 2-D space. In the following, we propose a separability metric based on linear separability~\cite{shamos1975geometric} for a 2-D visualization. 

Let us first review the concept of {\em linear separability} introduced in Euclidean geometry~\cite{shamos1975geometric}. A pair of point sets in two dimensions, i.e., $\oo_+$ and $\oo_-$, are \emph{linearly separable} if there exits at least one straight line in the plane such that all points from $\oo_+$ are on one side of the line, while all points from $\oo_-$ are on the other side of the line. Mathmatically, we can represent a line $\ell$ using Equation~\ref{eqn:line}, where $x$ and $y$ represent a object's value on feature $f_i$ and $f_j$ respectively, and $w_0$, $w_i$ and $w_j$ are coefficients. Given a feature pair $(f_i,f_j)$ and a line $\ell$, let $\eta_{i,j}^{\ell,k}$ be the estimated label of a object $o_k$. The estimated label $\eta_{i,j}^{\ell,k}$ is calculated according to Equation~\ref{eqn:est_label}: if $o_k$ lies on the upper side of $\ell$, then $\eta_{i,j}^{\ell,k}=1$; otherwise, $\eta_{i,j}^{\ell,k}=-1$.   
If there exists a line $\ell$ such that for any objects $o_k\in \widehat{\oo}$, the estimated label $\eta_{i,j}^{\ell,k}$ is consistent with the real label $l_k$ as shown in Equation~\ref{eqn:linear}, then we say $\oo_+$ and $\oo_-$ are linear separable. 
\begin{equation}\label{eqn:line}
\begin{split}
\ell: \hspace{4mm} w_i\cdot x + w_j\cdot y +w_0 =0 
\end{split}
\end{equation}
\begin{equation}\label{eqn:est_label}
\eta_{i,j}^{\ell,k}=\sign(w_i\cdot \mm_{i,k} + w_j\cdot \mm_{j,k} +w_0)
\end{equation}
\begin{equation}\label{eqn:linear}
\eta_{i,j}^{\ell,k} = \left\{
                \begin{array}{ll}
                  1 \textit{\hspace{2mm} if \hspace{2mm}} o_k\in \oo_+, \textit{ i.e., } l_k=1 \\
                  -1 \textit{\hspace{2mm} if \hspace{2mm}} o_k\in \oo_-, \textit{ i.e., } l_k=-1 
                \end{array}
              \right.
\end{equation}



Next, let us introduce our proposed separability metric. Our proposed separability metric is based on linear separability and is defined as {\em how well a 2-D visualization can be linearly separated}. This is because \genviz stresses on visual explainability and linear separability in a 2-D visualization can be easily recognized and interpreted by the users. We then formally define the separability metric.
First, given a feature pair $(f_i, f_j)$ and a line $\ell$ in the 2-D plane, a object $o_k$ is said to be correctly separated if Equation~\ref{eqn:linear} holds, i.e., $\eta_{i,j}^{\ell,k}\cdot l_k = 1$. Correspondingly, let $\theta_{i,j}^{\ell,k}$ be the variable indicating whether object $o_k$ is correctly separated, as depicted in Equation~\ref{eqn:s_object}. Then, given a feature pair $(f_i, f_j)$ and a line $\ell$, the separability score is defined as the number of correctly separated objects, denoted as $\theta_{i, j}^\ell$ as shown in Equation~\ref{eqn:s_line}. Figure~\ref{fig:brute_force} shows some possible $\theta_{i, j}^\ell$ with different separating lines. Finally, the separability score for a feature pair $(f_i,f_j)$ is defined as the largest $\theta_{i, j}^{\ell}$ among all possible lines $\ell$, denoted as $\theta_{i, j}$ as shown in Equation~\ref{eqn:s_viz}. 
\begin{equation}\label{eqn:s_object}
\theta_{i,j}^{\ell,k}=\left\{
                \begin{array}{ll}
                  1 \textit{\hspace{2mm} if \hspace{2mm}} \eta_{i,j}^{\ell,k}\cdot l_k = 1\\
                  0 \textit{\hspace{2mm} otherwise \hspace{2mm}} 
                \end{array}
              \right.
\end{equation}
\begin{equation}\label{eqn:s_line}
\theta_{i,j}^{\ell}= \sum_{k}{\theta_{i,j}^{\ell,k}}
\end{equation}
\begin{equation}\label{eqn:s_viz}
\theta_{i,j}= \max_{\ell}\{\theta_{i,j}^{\ell}\}
\end{equation}

%The basic intuition is that the users can easily recognize and interpret linear separability in a 2-D visualization. Thus if a visualization is perfectly linearly separated, it should have high score in our proposed separability metric; otherwise, we would find the best separating line $\ell$ in the visualization and report the largest number of correctly separated objects as the separability score.\silu{talk about weighted case}

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.235\textwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/metric.pdf}
		\vspace{-5mm}
		\caption{Brute Force}%{$\theta_{i,j}^{\ell}$ with Different Lines}
		\label{fig:brute_force}
	\end{subfigure}
	\begin{subfigure}{.235\textwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/rocchio.pdf}
		\vspace{-5mm}
		\caption{Rocchio-Based}%{$\theta_{i,j}^{\hat{\ell}}$ with Representative Line}
		\label{fig:rocchio}
	\end{subfigure}
\caption{Different methods to Calculate Separability Score $\theta_{i,j}$}
\label{fig:metric}
\end{figure} 



\stitle{Brute Force.} As illustrated in Figure~\ref{fig:brute_force}, a brute force way to calculate $\theta_{i,j}$ is to first enumerate all possible separating lines $\ell$ and calculate each $\theta_{i,j}^\ell$. This is infeasible as there are infinite number of possible lines. However, we can easily trim down the search space to $O(n^2)$ lines by linking every two points in the 2-D plane. This is because the results of all other possible lines can be covered by these $O(n^2)$ lines. Nevertheless, it is still very time-consuming to consider $O(n^2)$ lines for each feature pair $(f_i,f_j)$. We further propose to use a {\em representative line} $\hat{\ell}$ to approximate $\theta_{i, j}$ as shown in Equation~\ref{eqn:s_viz_appr}, instead of considering all $O(n^2)$ possible lines and pick the best in Equation~\ref{eqn:s_viz}. Now, the search space is reduced to $O(1)$ from $O(n^2)$. The representative line is picked based on Rocchio's algorithm~\cite{rocchio1971relevance}. As we will show later in Section~\ref{sec:exp}, $\theta_{i,j}^{\hat{\ell}}$ is comparable to $\theta_{i,j}$ when using Rocchio-based representative line $\hat{\ell}$. Let us describe in detail about the Rocchio-based representative line.
\begin{equation}\label{eqn:s_viz_appr}
\theta_{i,j}  \approx \theta_{i,j}^{\hat{\ell}}
\end{equation}

\stitle{Rocchio-based.} The basic idea of Rocchio's algorithm is to estimate each object's label the same as its nearest centroid. More specifically, let us denote the centroid of positive objects $\oo_+$ and negative objects $\oo_-$ as $\mu_+=(\mm_i^+,\mm_j^+)$ and $\mu_-=(\mm_i^-,\mm_j^-)$ respectively ,as shown in Figure~\ref{fig:rocchio}. Link $\mu_+$ with $\mu_-$, then the perpendicular bisector is defined as the representative separating line $\hat{\ell}$. Mathmatically, $\hat{\ell}$ can be represented as an instantiated line in Equation~\ref{eqn:line} with instantiated $w_i$, $w_j$ and $w_0$ as shown in Equation~\ref{eqn:rep_line}. Figure~\ref{fig:rocchio} illustrates the centroids $\mu_+$ and $\mu_-$, as well as the Rocchio-based representative separating line $\hat{\ell}$. The corresponding $\theta_{i,j}^{\hat{\ell}}$ equals 13 with one negative object (blue point in Figure~\ref{fig:rocchio}) mis-estimated as positive.


%\hat{\ell}: \hspace{4mm} (\mm_i^+-\mm_i^-)\cdot x + (\mm_j^+-\mm_j^-)\cdot y -(\frac{(\mm_i^+)^2-(\mm_i^-)^2}{2}+\frac{(\mm_j^+)^2-(\mm_j^-)^2}{2}) =0 

\begin{equation}\label{eqn:rep_line}
\begin{split}
& w_i = \mm_i^+-\mm_i^- \\
& w_j = \mm_j^+-\mm_j^- \\
& w_0 = -(\frac{(\mm_i^+)^2-(\mm_i^-)^2}{2}+\frac{(\mm_j^+)^2-(\mm_j^-)^2}{2})
\end{split}
\end{equation}


\stitle{Brute-force v.s. Rocchio-based.} Compared to brute force, Rocchio-based method is more light-weight in terms of running time, but at the cost of accuracy in calculating $\theta_{i,j}$. Intuitively, Rocchio-based representative line is a reasonable approxy to the best separating line since Rocchio-based method assigns each object to its nearest centroid and each centroid is calculated as the representative of each group. We will further experimentally demonstrate that $\theta_{i,j}-\theta_{i,j}^{\hat{\ell}}$ is small in Section~\ref{sec:exp}.


%==================================================================
\section{Proposed Optimizations}\label{sec:opt}
In the following, we will focus on Rocchio-based method to calculate the separability score. We first analyze the time complexity and then propose several optimization techniques in order to reduce the running time. 

\stitle{Time Complexity Analysis.} Given a feature pair $(f_i, f_j)$, the separating line $\hat{\ell}$ can be calculated in $O(1)$ using Equation~\ref{eqn:rep_line}. However, calculating the number of correctly separated objects $\theta_{i,j}^{\hat{\ell}}$ requires a full scan of all objects, i.e., $O(n)$. In addition, by choosing any two features from the whole feature set, there are $O(m^2)$ feature pair candidates. Thus, in total the running time complexity is $O(m^2n)$, which is very time-consuming especially when $m$ and $n$ are large. In the following, we propose different methodologies to reduce the running time.

First, we observe massive redundancy in calculating Equation~\ref{eqn:rep_line} for different feature pairs. In Section~\ref{ssec:trans}, we propose to map object-feature matrix $\mm$ into a different space, and then transform Equation~\ref{eqn:rep_line} accordingly. Next, we introduce several different optimization modulars to further reduce the running time in Section~\ref{ssec:earlyT},~\ref{ssec:sampling} and \ref{ssec:traversal}. Modular \earlyT and \sampling aim to reduce the number of objects checked (i.e., $n$), while modular \traversal aims to reduce the number of feature pairs checked (i.e., $m^2$). These optimization modulars can be used by their own or combined together, as we will show in Section~\ref{sec:exp}. 

\subsection{Pre-Transformation} \label{ssec:trans}
\stitle{Motivation.} Let us first review the process of computing the separability score $\theta_{i,j}^{\hat{\ell}}$. Given a fature pair $(f_i,f_j)$, we first compute $w_0$, $w_i$ and $w_j$ for $\hat{\ell}$ based on Equation~\ref{eqn:rep_line}. Next, for each object $o_k$, we can get the estimated label $\eta_{i,j}^{\hat{\ell},k}$ according to Equation~\ref{eqn:est_label}. This step requires two multiplications and three additions. Last, we can calculate $\theta_{i,j}^{\hat{\ell},k}$ and the separability score $\theta_{i,j}^{\hat{\ell}}$ based on Equation~\ref{eqn:s_object} and \ref{eqn:s_line} respectively. The whole process is repeated for every feature pair candidate. However, we observe that there exists massive redundancy when calculating different feature pairs. For instance, given two feature pairs $(f_i,f_j)$ and $(f_i,f_{j'})$ with fixed $f_i$, $w_i$ is in fact the same, and $w_i\cdot \mm_{i,k}$ in Equation~\ref{eqn:est_label} is repeated twice for each object $o_k$. Thus, we propose to pre-transform $\mm_{i,k}$ into another space to reduce the computational redundancy.

\stitle{Transformation.} For each $\mm_{i,k}$, i.e., the value of object $o_k$ on feature $f_i$, we pre-transform it to $\widehat{\mm}_{i,k}$ as illustrated in Equation~\ref{eqn:matrix_transform}, where $\mm_i^+$ ($\mm_j^+$) and $\mm_i^-$ ($\mm_j^-$) are constants given a feature $f_i$ ($f_j$). By incorporating $\ell_k$ in Equation~\ref{eqn:matrix_transform}, we can unify the process of calculating $\theta_{i,j}^{\hat{\ell},k}$ for both positive and negative objects, and get rid of checking the real label $\ell_k$ in Equation~\ref{eqn:s_object}. Accordingly, Equation~\ref{eqn:est_label} and Equation~\ref{eqn:s_object} can be combined and transformed into Equation~\ref{eqn:s_object_transform}.

\begin{equation}\label{eqn:matrix_transform}
\widehat{\mm}_{i,k} = ((\mm_i^+-\mm_i^-)\cdot \mm_{i,k}-\frac{(\mm_i^+)^2-(\mm_i^-)^2}{2})\cdot l_k 
\end{equation}

\begin{equation}\label{eqn:s_object_transform}
\theta_{i,j}^{\hat{\ell},k}=\left\{
                \begin{array}{ll}
                  1 \textit{\hspace{2mm} if \hspace{2mm}} \sign(\widehat{\mm}_{i,k} + \widehat{\mm}_{j,k}) = 1\\
                  0 \textit{\hspace{2mm} otherwise \hspace{2mm}} 
                \end{array}
              \right.
\end{equation}

%\begin{equation}\label{eqn:est_label_transform}
%\eta_{i,j}^{\hat{\ell},k}=\sign(\widehat{\mm}_{i,k} + \widehat{\mm}_{j,k})
%\end{equation}

Now, let us revisit the process of computing separability score $\theta_{i,j}^{\hat{\ell}}$ after pre-transformation. Given a feature pair $(f_i,f_j)$, we can compute $\theta_{i,j}^{\hat{\ell},k}$ for each object $o_k$ based on Equation~\ref{eqn:s_object_transform}. Note that this step only involves one addition and one comparison. Then, similar to that without transformation, we can calculate $\theta_{i,j}^{\hat{\ell}}$ based on Equation~\ref{eqn:s_line}. In all, compared to that without transformation, we not only eliminate the step of computing $w_0$, $w_i$ and $w_j$ for every feature pair, but also reduce the cost of calculating $\eta_{i,j}^{\hat{\ell},k}$ in Equation~\ref{eqn:est_label}. Hence, by conducting transformation in Equation~\ref{eqn:matrix_transform} and~\ref{eqn:s_object_transform}, we have mapped our problem to a new space. In the following sections, we will consider $\widehat{\mm}$ instead of $\mm$, and Equation~\ref{eqn:s_object_transform} instead of Equation~\ref{eqn:est_label} and~\ref{eqn:s_object}.


\begin{example}[Transformation]
Figure~\ref{fig:transform} illustrates the transformation for two feature $f_i$ and $f_j$. The upper part depicts $\mm_{i,k}$ and $\mm_{j,k}$ before transformation, where red color represents positive label and green color represents negative labels. As we can see, the centroid of $\oo_+$ and $\oo_-$ are $\mu_+=(5,7)$ and $\mu_-=(3,5)$ respectively. Hence, we can rewrite Equation~\ref{eqn:matrix_transform} as $\widehat{\mm_{i,k}} = (2\mm_{i,k}-8)\cdot l_k$ and $\widehat{\mm_{j,k}} = (2\mm_{i,k}-12)\cdot l_k$ for feature $f_i$ and $f_j$ respectively. After calculation, we can obtain the $\widehat{\mm_{i,k}}$ and $\widehat{\mm_{j,k}}$ in the lower part of Figure~\ref{fig:transform}.
\end{example}

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.235\textwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/transformation.pdf}
		\vspace{-5mm}
		\caption{Pre-Transformation}%{$\theta_{i,j}^{\ell}$ with Different Lines}
		\label{fig:transform}
	\end{subfigure}
	\begin{subfigure}{.235\textwidth}
		\centering
		\includegraphics[width=\linewidth]{fig/earlyT.pdf}
		\vspace{-5mm}
		\caption{Early Termination ($\tau = 1$)}%{$\theta_{i,j}^{\hat{\ell}}$ with Representative Line}
		\label{fig:earlyT}
	\end{subfigure}
\caption{Different Methods to Reduce Running Time}
\label{fig:metric}
\end{figure} 

\stitle{Remark.} For a feature pair $(f_i,f_j)$ where $i=j$, we can treat the corresponding separability score $\theta_{i,i}$ as the single separability score for feature $f_i$. Equation~\ref{eqn:s_object_transform} can be further reduced to Equation~\ref{eqn:s_object_transform_single}. Hence, we can calculate the single separability score $\theta_{i,i}$ very efficiently based on the transformed matrix $\widehat{\mm}$.
\begin{equation}\label{eqn:s_object_transform_single}
\theta_{i,i}^{\hat{\ell},k}=\left\{
                \begin{array}{ll}
                  1 \textit{\hspace{2mm} if \hspace{2mm}} \sign(\widehat{\mm}_{i,k}) = 1\\
                  0 \textit{\hspace{2mm} otherwise \hspace{2mm}} 
                \end{array}
              \right.
\end{equation}


\subsection{Early Termination} \label{ssec:earlyT}
As discussed at the beginning of Section~\ref{sec:opt}, given a feature pair $(f_i,f_j)$, we need to make a full scan of all objects to compute $\theta_{i,j}^{\hat{\ell}}$. However, we are not concerned about all feature pairs' separability score, instead we only care about the \topk feature pairs. Thus, we propose to reduce the running time using early termination when scanning the objects, denoted as \earlyT modular as shown in Figure~\ref{fig:workflow}.

\stitle{High Level Idea.} The basic idea is to maintain a upper bound for the separability error of the \topk feature pairs, denoted as $\tau$. Correspondingly, the lower bound of separability score can be denoted as ($n-\tau$), where $n$ is the total number of objects $\hat{\oo}$. Given a feature pair $(f_i,f_j)$, we start to scan the object list until the number of mistakenly separated objects exceeds $\tau$, then terminate and abandom this feature pair since it can not be among the \topk feature pairs; otherwise, $(f_i,f_j)$ is added to the feature pair candidate set and we update $\tau$ accordingly.

\stitle{Enhancement by object Ordering.} Furthermore, in order to terminate early, an intuitive method is to first examine the objects that are more problematic. In this way, given a bad feature pair, the number of mistakenly separated objects is likely to fast exceeds ($n-\tau$) by just looking at a few objects. Hence, we propose to order the objects in descending order in terms of "problematic". In our experiment, for each object $o_k$, we quantify "problematic" by the number of single features $f_i$ that mistakendly separate this object, i.e., $\widehat{\mm_{i,k}}<=0$.

\begin{example}[\earlyT \& object Ordering]
Figure~\ref{fig:earlyT} illustrates how early termination and object ordering help reduce the running time. The upper part is $\widehat{\mm}$ after transformation. Assume the current error upper bound is $\tau=1$. We start scan the object list from left to right and determine whether each object is correctly separated or not based on Equation~\ref{eqn:s_object_transform}. object $o_1$ is mistakenly separated since $\widehat{\mm_{i,1}}+\widehat{\mm_{j,1}}\leq0$, marked as blue color in Figure~\ref{fig:earlyT}. We can terminate at $o_5$ since the number of mistakenly separated objects exceeds $\tau=1$.

The lower part of Figure~\ref{fig:earlyT} shows the $\widehat{\mm}$ after object reordering, where "problematic" objects are placed on the front. In this way, by only checking the first two objects, we can terminate and get rid of this feature pair.
\end{example}

\subsection{Sampling-based Estimation} \label{ssec:sampling}
We have proposed \earlyT modular in Section~\ref{ssec:earlyT} to reduce the number of objects checked (i.e., $n$). However, there is no guarantee for the running time performance since the termination point is very data-dependent. In this section, we propose a schochastic method to reduce the number of objects, called \sampling. Instead of calculating $\theta_{i,j}^{\hat{\ell}}$ over the whole object set $\hat{\oo}$, \sampling works on a sample set drawing from $\hat{\oo}$.

\stitle{High Level Idea.} At a high level, \sampling mainly consists of two phases, called {\em candidate generation} and {\em candidate validation} as shown in Figure~\ref{fig:sampling}. At phase one (i.e., Figure~\ref{fig:sampling}(a)), we first estimate $\theta_{i,j}$ over a sample set, and then generate the feature pair candidate set based on each feature pair's confidence interval. Next, at phase two (i.e., Figure~\ref{fig:sampling}(b)), we only re-evaluate the feature pairs in the candidate set, calculate $\theta_{i,j}$ over the whole object set $\hat{\oo}$, and then obtain the \topk feature pairs. We will describe in detail in Section~\ref{sssec:generate} and \ref{sssec:validate}.
\begin{figure}[t!]
  \centering
  \vspace{-2mm}
  \includegraphics[width=\linewidth]{fig/sampling.pdf}
  \vspace{-6mm}
\caption{\sampling (TOP-3 Feature Pairs)}
\label{fig:sampling}
\end{figure} 
% \begin{figure}[h]
%   \centering
%   \begin{subfigure}{0.235\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/phase1.pdf}
%     \vspace{-5mm}
%     \caption{Candidate Generation}%{$\theta_{i,j}^{\ell}$ with Different Lines}
%     \label{fig:phase1}
%   \end{subfigure}
%   \begin{subfigure}{.235\textwidth}
%     \centering
%     \includegraphics[width=\linewidth]{fig/phase2.pdf}
%     \vspace{-5mm}
%     \caption{Candidate Validation)}%{$\theta_{i,j}^{\hat{\ell}}$ with Representative Line}
%     \label{fig:phase2}
%   \end{subfigure}
% \caption{\sampling (TOP-3)}
% \label{fig:sampling}
% \end{figure} 
\subsubsection{Candidate Generation}\label{sssec:generate}
Let $\sss$ be the sample set, drawing uniformly from the object set $\hat{\oo}$. Given a feature pair $(f_i,f_j)$, let $\theta_{i,j}(\sss)$ be the number of correctly separated samples over $\sss$. Let $\tilde{\theta}_{i,j}$ be the estimated separating score based on the sample set $\sss$. First, we can estimate $\tilde{\theta}_{i,j}$ from $\theta_{i,j}(\sss)$ usinng Equation~\ref{eqn:est_score}: the ratio of correctly separated samples is the same as nthat in $\hat{\oo}$. 

\begin{equation}\label{eqn:est_score}
\tilde{\theta}_{i,j} = \frac{\theta_{i,j}(\sss)}{|\sss|} \cdot n
\end{equation}

Next, we will show that with a constant size of the sample set, $|\theta_{i,j}-\tilde{\theta}_{i,j}|$ is bounded by a small value with high probability. 

\begin{theorem}[Estimation Accuracy~\cite{acharya2015fast}]\label{them:est}
By considering $\Omega(\frac{1}{\epsilon^2}\cdot \log(\frac{1}{\delta}))$ samples, with probability at least $1-\delta$, we have $|\frac{\theta_{i,j}(\sss)}{|\sss|}-\frac{\theta_{i,j}}{n}| \leq \epsilon$, i.e., $|\tilde{\theta}_{i,j}-\theta_{i,j}|\leq \epsilon n$.
\end{theorem}

We can treat $log(1/\delta)$ as a constant, e.g., by setting $\delta = 0.05$.Thus, Theorem~\ref{them:est} essentially states that with only $\Omega(\frac{1}{\epsilon^2})$ samples, w.h.p. the confidence interval for $\theta_{i,j}$ is $[\tilde{\theta}_{i,j}-\epsilon n, \tilde{\theta}_{i,j}+\epsilon n]$. Note that the sample size $|\sss|$ only relies on $\epsilon$ and is independent of the object size $n$. Hence, the \sampling modular helps \genviz to scale to datasets with large $n$. 
%For instance, when setting $\epsilon=0.05$, the sample size is fixed to be $400$. 

With Theorem~\ref{them:est}, we can first calculate the confidence interval of $\theta_{i,j}$ for each feature pair $(f_i,f_j)$, as shown in Figure~\ref{fig:sampling}(a). Next, we compute the lower bound of $\theta_{i,j}$ for the \topk feature pairs, denoted as $\zeta$ as shown by the red dotted line in Figure~\ref{fig:sampling}(a). At last, we can prune away feature pairs whose upper bound is smaller than $\zeta$. Correspondingly, we can generate the feature pair, denoted as $\cc$ candidates as depicted by the blue intervals in Figure~\ref{fig:sampling}(a). These feature pairs $\cc$ will be further validated in phase two, i.e., candidate validation. Typically, $|\cc|$ is orders of magnitude smaller than $m^2$, the original search space for all feature pairs. We will verify this later in our experiments (Section~\ref{sec:exp}). 

\begin{example}[Candidate Generation]
Let us illustrate phase one using Figure~\ref{fig:sampling}(a). Set $\epsilon = 0.05$ and $K=3$. We sample $\Omega(\frac{1}{\epsilon^2})=400$ objects and compute the confidence interval of $\theta_{i,j}$ for each $(f_i,f_j)$. Next, we obtain the $3^{th}$ lower bound $\zeta$ (red dotted line) in ascending order among all the confidence intervals. At last, we generate the feature pair candidates $\cc$, whose upper bound is larger than the red dotted line.
\end{example}

\subsubsection{Candidate Validation}\label{sssec:validate}
In phase two, we re-evaluate all the feature pair candidates generated from phase 1. The evaluation works on the whole object set $\hat{\oo}$. For each feature pair $(f_i,f_j)\in \cc$, we calculate the real separability score $\theta_{i,j}$, and find the \topk feature pairs as shown in Figure~\ref{fig:sampling}(b). 

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{fig/candidate_ordering.pdf}
  \vspace{-6mm}
\caption{Enhancement by Candidate Ordering}
\label{fig:candidate_ordering}
\end{figure} 


\stitle{Enhancement by Candidate Ordering.}  Instead of naively validating each feature pair candidate, we further propose to first order the candidates in descending order of the upper bound of each confidence interval. Then, we sequentially calculate the real separability score $\theta_{i,j}$ for each feature pair, and update $\zeta$ correspondingly. At last, we terminate when the feature pair's upper bound is even smaller than the current $\zeta$.


\begin{example}[Candidate Ordering]
We illustrate it using Figure~\ref{fig:candidate_ordering}. Initially, the lower bound of the \topthree feature pairs, i.e., $\zeta$, is depicted using the red dotted line  in Figure~\ref{fig:candidate_ordering}(a). We first sort the feature pair candidates in descending order of the upper bound (red line). Next, we calculate $\theta_{i,j}$ for the first feature pair, as shown by the blue ball in Figure~\ref{fig:candidate_ordering}(b), with $\zeta$ unchanged. Similarly, we calculate $\theta_{i,j}$ for the second and third feature pairs, as shown in Figure~\ref{fig:candidate_ordering}(c) and (d), and update $\zeta$ accordingly. Note that $\zeta$ (the red dotted line) is moved forward in Figure~\ref{fig:candidate_ordering}(d). Since $\zeta$ is already larger than the upper bound of the following feature pair, we can terminate.
\end{example}

\subsection{Search Space Traversal} \label{ssec:traversal}
In Section~\ref{ssec:earlyT} and \ref{ssec:sampling}, we have discussed how to reduce the number of objects checked (i.e., $n$). The question followed is that can we reduce the number of feature pair considered (i.e., $m^2$). In this section, we focus on how to smartly traverse the search space if allowed only a limited number of feature pairs, called \traversal Modular. 

\stitle{High Level Idea.}  The basic idea is that instead of examining each possible feature pair, we only check a limited number of feature pairs, but in a {\em smarter traversal order}. The number of checked feature pairs determines a trade-off between the efficiency and the accuracy. In general, the less number of feature pairs checked, the faster in terms of the running time, however, at the cost of accuracy in terms of the output \topk feature pairs.

The full search space for the feature pairs is the upper triangle in Figure~\ref{fig:traversal}(a), where each row represents $f_i$ and each column represents $f_j$. Let $\chi$ be the limited number of feature pairs allowed. We propose two different traversal order over the search space: horizontal traversal and vertical traversal. In the following, we will describe them in detail.
\begin{itemize}
\item \emph{Horizontal traversal (Figure~\ref{fig:traversal}(a)):} for each feature $f_i$, pair it with all other feature $f_j$, where $j\geq i$, and obtain $(f_i,f_j)$. Repeat it for each $f_i$, where $1 \leq i\leq m$.
\item \emph{Vertical traversal (Figure~\ref{fig:traversal}(b)):} for each feature $f_j$, pair it with all other feature $f_i$, where $i\leq j$, and obtain $(f_i,f_j)$. Repeat it for each $f_j$, where $1 \leq j\leq m$.
%\item \emph{Diagonal traversal (Figure~\ref{fig:traversal}(c)):} for a fixed value $v$, pair $(f_i,f_{v-i})$, where $1\leq i<v$. Repeat it for each $v$, where $2 \leq v\leq 2m$.
\end{itemize}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.85\linewidth]{fig/traversal.pdf}
  \vspace{-3mm}
\caption{Different Traversal Ordering}
\label{fig:traversal}
\end{figure} 

However, the traversal order does not convey any intuition if the feature set $\ff$ is in random order. Hence, we further propose to enhance \traversal by feature ordering in $\ff$.

\stitle{Enhancement by Feature Ordering.} We propose to first sort features based on single feature's separability score $\theta_{i,i}$. Single features performs good by their own, i.e., with high separability score, are ranked in the front. Let $\{f_1^{'} f_2^{'},\cdots,f_m^{'}\}$ be the sorted single feature set. First, we coarsely classify the sorted feature set into three categories based on the single feature separability score $\theta_{i,i}$: {\em top, median,} and {\em bad}. Next, we discuss the intuition behind different traversal order:

\begin{itemize}
\item \emph{Horizontal traversal:} the intuition is that there is at least one {\em top} or {\em median} single feature in each \topk feature pair $(f_i,f_j)$.
\item \emph{Vertical traversal:}  the intuition is that both features in each \topk feature pair $(f_i,f_j)$ must rank {\em top} or {\em median} by their own.
%\item \emph{Diagonal traversal:} 
\end{itemize}


%The intuition behind horizontal traversal is that there is at least one {\em top} or {\em median} single features in the \topk feature pairs $(f_i,f_j)$; while the intuition behind  horizontal traversal is that both features in \topk feature pairs $(f_i,f_j)$ must rank {\em top} or {\em median} by their own.

%Typically, $K$ is not large, e.g., {\sc Top-1000}, while $m$ is large, e.g., {\sc 20000}. 
\begin{example}[\traversal]
Assume $m=2\times 10^4$ and $K=100$. Set $\chi=10^7$. Initially, the number of possible feature pairs is $\frac{m^2}{2}=2\times 10^8$; while now we reduce the whole search space to  $\frac{1}{20}$ of that,  i.e., $\chi=2\times 10^6$. First, single features are sorted in descending order of the separability score. Next, in horizontal traversal, the "worst" $f_i$ ranks around 500, while $f_j$ can be any feature in $\ff$. On the other hand, in vertical traversal, the "worst" $f_i$ (and $f_j$) ranks around 2000.
\end{example}
%==================================================================
\section{Experiment}
\label{sec:exp}








